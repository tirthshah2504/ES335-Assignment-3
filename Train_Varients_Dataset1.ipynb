{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports and utilities\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_44749/1603823187.py:6: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  cleaned_text = re.sub('[^a-zA-Z0-9 \\.]', ' ', text)\n"
     ]
    }
   ],
   "source": [
    "# Read and preprocess the text\n",
    "with open('Sherlock.csv', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Remove special characters except alphanumeric, space, and full stop\n",
    "cleaned_text = re.sub('[^a-zA-Z0-9 \\.]', ' ', text)\n",
    "\n",
    "# Convert to lowercase\n",
    "cleaned_text = cleaned_text.lower()\n",
    "\n",
    "# Split into sentences using full stop\n",
    "sentences = [s.strip() for s in cleaned_text.split('.') if s.strip()]\n",
    "\n",
    "# Extract all words\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    words.extend(sentence.split())\n",
    "\n",
    "# Create vocabulary from unique words\n",
    "vocabulary = set(words)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mappings for 8150 words\n"
     ]
    }
   ],
   "source": [
    "# Create word to index and index to word mappings\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(f\"Created mappings for {len(word_to_idx)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 109145 training sequences\n",
      "Sequence shape: torch.Size([109145, 3])\n",
      "Target shape: torch.Size([109145])\n",
      "\n",
      "Training samples: 87316\n",
      "Validation samples: 21829\n"
     ]
    }
   ],
   "source": [
    "# Create training sequences (context window = 3 words to predict next word)\n",
    "def create_sequences(words, window_size=3):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(words) - window_size):\n",
    "        seq = words[i:i+window_size]\n",
    "        target = words[i+window_size]\n",
    "        \n",
    "        # Convert to indices\n",
    "        seq_indices = [word_to_idx[word] for word in seq]\n",
    "        target_idx = word_to_idx[target]\n",
    "        \n",
    "        sequences.append(seq_indices)\n",
    "        targets.append(target_idx)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "window_size = 3\n",
    "X, y = create_sequences(words, window_size)\n",
    "X = torch.tensor(X).to(device)\n",
    "y = torch.tensor(y).to(device)\n",
    "\n",
    "print(f\"Created {len(X)} training sequences\")\n",
    "print(f\"Sequence shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP Text Generator Model with configurable activation function\n",
    "class MLPTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, window_size, \n",
    "                 activation_fn, dropout_prob=0.3):\n",
    "        super(MLPTextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.input_dim = embedding_dim * window_size\n",
    "        \n",
    "        # Map string to actual activation function\n",
    "        self.activation = self._get_activation(activation_fn)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def _get_activation(self, activation_fn):\n",
    "        \"\"\"Return activation function module given string name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "        }\n",
    "        if activation_fn.lower() not in activations:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_fn}\")\n",
    "        return activations[activation_fn.lower()]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, window_size)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(x)  # (batch_size, window_size, embedding_dim)\n",
    "        \n",
    "        # Flatten embeddings\n",
    "        embedded = embedded.view(batch_size, -1)  # (batch_size, window_size * embedding_dim)\n",
    "        \n",
    "        # MLP forward\n",
    "        h1 = self.dropout1(self.activation(self.fc1(embedded)))\n",
    "        output = self.fc2(h1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8,713,878\n",
      "Trainable parameters: 8,713,878\n",
      "\n",
      "Model architecture:\n",
      "MLPTextGenerator(\n",
      "  (embedding): Embedding(8150, 32)\n",
      "  (activation): Tanh()\n",
      "  (fc1): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=8150, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model1 = MLPTextGenerator(vocab_size, embedding_dim=32, hidden_dim=1024, window_size=3, activation_fn='tanh')\n",
    "model1 = model1.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model1.parameters())\n",
    "trainable_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\\n{model1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 9,072,982\n",
      "Trainable parameters: 9,072,982\n",
      "\n",
      "Model architecture:\n",
      "MLPTextGenerator(\n",
      "  (embedding): Embedding(8150, 64)\n",
      "  (activation): Tanh()\n",
      "  (fc1): Linear(in_features=192, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=8150, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model2 = MLPTextGenerator(vocab_size, embedding_dim=64, hidden_dim=1024, window_size=3, activation_fn='tanh')\n",
    "model2 = model2.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model2.parameters())\n",
    "trainable_params = sum(p.numel() for p in model2.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\\n{model2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_epoch(model, X_train, y_train, optimizer, criterion, batch_size=128):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "    \n",
    "    num_batches = len(X_train) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        batch_X = torch.LongTensor(X_train_shuffled[start_idx:end_idx]).to(device)\n",
    "        batch_y = torch.LongTensor(y_train_shuffled[start_idx:end_idx]).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation function\n",
    "def validate(model, X_val, y_val, criterion, batch_size=128):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    num_batches = len(X_val) // batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            batch_X = torch.LongTensor(X_val[start_idx:end_idx]).to(device)\n",
    "            batch_y = torch.LongTensor(y_val[start_idx:end_idx]).to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[model1, model2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500]\n",
      "  Train Loss: 0.9398 | Train Acc: 73.05%\n",
      "  Val Loss:   9.3789 | Val Acc:   12.81%\n",
      "============================================================\n",
      "Epoch [100/500]\n",
      "  Train Loss: 0.9183 | Train Acc: 73.47%\n",
      "  Val Loss:   9.4202 | Val Acc:   12.79%\n",
      "============================================================\n",
      "Epoch [150/500]\n",
      "  Train Loss: 0.9174 | Train Acc: 73.43%\n",
      "  Val Loss:   9.4566 | Val Acc:   12.77%\n",
      "============================================================\n",
      "Epoch [200/500]\n",
      "  Train Loss: 0.8882 | Train Acc: 74.33%\n",
      "  Val Loss:   9.4795 | Val Acc:   12.87%\n",
      "============================================================\n",
      "Epoch [250/500]\n",
      "  Train Loss: 0.8827 | Train Acc: 74.55%\n",
      "  Val Loss:   9.5119 | Val Acc:   12.81%\n",
      "============================================================\n",
      "Epoch [300/500]\n",
      "  Train Loss: 0.8672 | Train Acc: 74.91%\n",
      "  Val Loss:   9.5310 | Val Acc:   12.62%\n",
      "============================================================\n",
      "Epoch [350/500]\n",
      "  Train Loss: 0.8589 | Train Acc: 75.07%\n",
      "  Val Loss:   9.5476 | Val Acc:   12.78%\n",
      "============================================================\n",
      "Epoch [400/500]\n",
      "  Train Loss: 0.8455 | Train Acc: 75.43%\n",
      "  Val Loss:   9.5632 | Val Acc:   12.61%\n",
      "============================================================\n",
      "Epoch [450/500]\n",
      "  Train Loss: 0.8319 | Train Acc: 75.81%\n",
      "  Val Loss:   9.5784 | Val Acc:   12.70%\n",
      "============================================================\n",
      "Epoch [500/500]\n",
      "  Train Loss: 0.8151 | Train Acc: 76.32%\n",
      "  Val Loss:   9.5843 | Val Acc:   12.63%\n",
      "============================================================\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: inf\n",
      "Starting training...\n",
      "============================================================\n",
      "Epoch [50/500]\n",
      "  Train Loss: 1.8192 | Train Acc: 53.15%\n",
      "  Val Loss:   7.4504 | Val Acc:   12.87%\n",
      "============================================================\n",
      "Epoch [100/500]\n",
      "  Train Loss: 1.4863 | Train Acc: 59.91%\n",
      "  Val Loss:   7.8692 | Val Acc:   12.78%\n",
      "============================================================\n",
      "Epoch [150/500]\n",
      "  Train Loss: 1.2580 | Train Acc: 64.95%\n",
      "  Val Loss:   8.2234 | Val Acc:   12.81%\n",
      "============================================================\n",
      "Epoch [200/500]\n",
      "  Train Loss: 1.0984 | Train Acc: 68.67%\n",
      "  Val Loss:   8.5177 | Val Acc:   12.90%\n",
      "============================================================\n",
      "Epoch [250/500]\n",
      "  Train Loss: 0.9869 | Train Acc: 71.44%\n",
      "  Val Loss:   8.7690 | Val Acc:   12.91%\n",
      "============================================================\n",
      "Epoch [300/500]\n",
      "  Train Loss: 0.8903 | Train Acc: 74.08%\n",
      "  Val Loss:   8.9805 | Val Acc:   12.63%\n",
      "============================================================\n",
      "Epoch [350/500]\n",
      "  Train Loss: 0.8234 | Train Acc: 75.84%\n",
      "  Val Loss:   9.1563 | Val Acc:   12.74%\n",
      "============================================================\n",
      "Epoch [400/500]\n",
      "  Train Loss: 0.7765 | Train Acc: 77.06%\n",
      "  Val Loss:   9.3057 | Val Acc:   12.68%\n",
      "============================================================\n",
      "Epoch [450/500]\n",
      "  Train Loss: 0.7381 | Train Acc: 78.21%\n",
      "  Val Loss:   9.4145 | Val Acc:   12.82%\n",
      "============================================================\n",
      "Epoch [500/500]\n",
      "  Train Loss: 0.7076 | Train Acc: 79.26%\n",
      "  Val Loss:   9.5172 | Val Acc:   12.87%\n",
      "============================================================\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: inf\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "i=1\n",
    "for model in models:\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 500\n",
    "    batch_size = 16384\n",
    "\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, X_train.cpu().numpy(), y_train.cpu().numpy(), optimizer, criterion, batch_size)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, X_val.cpu().numpy(), y_val.cpu().numpy(), criterion, batch_size)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), f'mlp_text_model_dataset1var{i}.pth')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 109143 training sequences\n",
      "Sequence shape: torch.Size([109143, 5])\n",
      "Target shape: torch.Size([109143])\n",
      "\n",
      "Training samples: 87314\n",
      "Validation samples: 21829\n"
     ]
    }
   ],
   "source": [
    "# Create training sequences (context window = 3 words to predict next word)\n",
    "def create_sequences(words, window_size):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(words) - window_size):\n",
    "        seq = words[i:i+window_size]\n",
    "        target = words[i+window_size]\n",
    "        \n",
    "        # Convert to indices\n",
    "        seq_indices = [word_to_idx[word] for word in seq]\n",
    "        target_idx = word_to_idx[target]\n",
    "        \n",
    "        sequences.append(seq_indices)\n",
    "        targets.append(target_idx)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "window_size = 5\n",
    "X, y = create_sequences(words, window_size)\n",
    "X = torch.tensor(X).to(device)\n",
    "y = torch.tensor(y).to(device)\n",
    "\n",
    "print(f\"Created {len(X)} training sequences\")\n",
    "print(f\"Sequence shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 9,204,054\n",
      "Trainable parameters: 9,204,054\n",
      "\n",
      "Model architecture:\n",
      "MLPTextGenerator(\n",
      "  (embedding): Embedding(8150, 64)\n",
      "  (activation): ReLU()\n",
      "  (fc1): Linear(in_features=320, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=8150, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model3 = MLPTextGenerator(vocab_size, embedding_dim=64, hidden_dim=1024, window_size=5, activation_fn='relu')\n",
    "model3 = model3.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model3.parameters())\n",
    "trainable_params = sum(p.numel() for p in model3.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\\n{model3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 9,204,054\n",
      "Trainable parameters: 9,204,054\n",
      "\n",
      "Model architecture:\n",
      "MLPTextGenerator(\n",
      "  (embedding): Embedding(8150, 64)\n",
      "  (activation): Tanh()\n",
      "  (fc1): Linear(in_features=320, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=8150, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model4 = MLPTextGenerator(vocab_size, embedding_dim=64, hidden_dim=1024, window_size=5, activation_fn='tanh')\n",
    "model4 = model4.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model4.parameters())\n",
    "trainable_params = sum(p.numel() for p in model4.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\\n{model4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8,779,414\n",
      "Trainable parameters: 8,779,414\n",
      "\n",
      "Model architecture:\n",
      "MLPTextGenerator(\n",
      "  (embedding): Embedding(8150, 32)\n",
      "  (activation): ReLU()\n",
      "  (fc1): Linear(in_features=160, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=8150, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model5= MLPTextGenerator(vocab_size, embedding_dim=32, hidden_dim=1024, window_size=5, activation_fn='relu')\n",
    "model5 = model5.to(device)\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model5.parameters())\n",
    "trainable_params = sum(p.numel() for p in model5.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\\n{model5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[model3, model4, model5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500]\n",
      "  Train Loss: 1.1722 | Train Acc: 68.34%\n",
      "  Val Loss:   8.8488 | Val Acc:   11.70%\n",
      "============================================================\n",
      "Epoch [200/500]\n",
      "  Train Loss: 0.6006 | Train Acc: 82.31%\n",
      "  Val Loss:   9.9535 | Val Acc:   11.38%\n",
      "============================================================\n",
      "Epoch [300/500]\n",
      "  Train Loss: 0.3525 | Train Acc: 89.20%\n",
      "  Val Loss:   10.9073 | Val Acc:   11.43%\n",
      "============================================================\n",
      "Epoch [400/500]\n",
      "  Train Loss: 0.2327 | Train Acc: 92.87%\n",
      "  Val Loss:   11.6805 | Val Acc:   11.19%\n",
      "============================================================\n",
      "Epoch [500/500]\n",
      "  Train Loss: 0.1641 | Train Acc: 94.91%\n",
      "  Val Loss:   12.3823 | Val Acc:   11.22%\n",
      "============================================================\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: inf\n",
      "Starting training...\n",
      "============================================================\n",
      "Epoch [100/500]\n",
      "  Train Loss: 1.5554 | Train Acc: 61.21%\n",
      "  Val Loss:   7.2607 | Val Acc:   11.02%\n",
      "============================================================\n",
      "Epoch [200/500]\n",
      "  Train Loss: 0.7210 | Train Acc: 79.91%\n",
      "  Val Loss:   8.3111 | Val Acc:   11.02%\n",
      "============================================================\n",
      "Epoch [300/500]\n",
      "  Train Loss: 0.4029 | Train Acc: 88.05%\n",
      "  Val Loss:   9.1469 | Val Acc:   11.11%\n",
      "============================================================\n",
      "Epoch [400/500]\n",
      "  Train Loss: 0.2672 | Train Acc: 92.03%\n",
      "  Val Loss:   9.7975 | Val Acc:   11.19%\n",
      "============================================================\n",
      "Epoch [500/500]\n",
      "  Train Loss: 0.2014 | Train Acc: 93.81%\n",
      "  Val Loss:   10.3011 | Val Acc:   11.25%\n",
      "============================================================\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: inf\n",
      "Starting training...\n",
      "============================================================\n",
      "Epoch [100/500]\n",
      "  Train Loss: 1.8208 | Train Acc: 54.21%\n",
      "  Val Loss:   8.1407 | Val Acc:   11.99%\n",
      "============================================================\n",
      "Epoch [200/500]\n",
      "  Train Loss: 1.2780 | Train Acc: 64.95%\n",
      "  Val Loss:   9.0007 | Val Acc:   12.12%\n",
      "============================================================\n",
      "Epoch [300/500]\n",
      "  Train Loss: 0.9660 | Train Acc: 72.20%\n",
      "  Val Loss:   9.6789 | Val Acc:   11.78%\n",
      "============================================================\n",
      "Epoch [400/500]\n",
      "  Train Loss: 0.7596 | Train Acc: 77.30%\n",
      "  Val Loss:   10.3082 | Val Acc:   11.80%\n",
      "============================================================\n",
      "Epoch [500/500]\n",
      "  Train Loss: 0.6214 | Train Acc: 81.23%\n",
      "  Val Loss:   10.8884 | Val Acc:   11.52%\n",
      "============================================================\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: inf\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "i=3\n",
    "for model in models:\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 500\n",
    "    batch_size = 16384\n",
    "\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, X_train.cpu().numpy(), y_train.cpu().numpy(), optimizer, criterion, batch_size)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, X_val.cpu().numpy(), y_val.cpu().numpy(), criterion, batch_size)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), f'mlp_text_model_dataset1var{i}.pth')\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
