{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports and utilities\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 113644\n",
      "\n",
      "Total words in corpus: 759639\n",
      "\n",
      "==================================================\n",
      "10 Most Frequent Words:\n",
      "==================================================\n",
      "*                    :  33504\n",
      "=                    :  28003\n",
      "{                    :  18915\n",
      "if                   :  17702\n",
      "}                    :  16965\n",
      "the                  :  16080\n",
      "*/                   :  13445\n",
      "/*                   :  12190\n",
      "struct               :  10997\n",
      "return               :  10130\n",
      "\n",
      "==================================================\n",
      "10 Least Frequent Words:\n",
      "==================================================\n",
      "context_tracking_init(void) :      1\n",
      "CONFIG_CONTEXT_TRACKING_FORCE :      1\n",
      "set_tsk_thread_flag(next, :      1\n",
      "clear_tsk_thread_flag(prev, :      1\n",
      "__context_tracking_task_switch(struct :      1\n",
      "TIF                  :      1\n",
      "syscalls.            :      1\n",
      "user-kernel          :      1\n",
      "__context_tracking_task_switch :      1\n",
      "NOKPROBE_SYMBOL(context_tracking_user_exit); :      1\n",
      "\n",
      "==================================================\n",
      "Vocabulary data saved to 'vocab_data1.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Read and preprocess the text\n",
    "with open('Linux.csv', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split into sentences using full stop\n",
    "sentences = [s.strip() for s in text.split('\\n') if s.strip()]\n",
    "\n",
    "# Extract all words\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    words.extend(sentence.split())\n",
    "\n",
    "# Create vocabulary from unique words\n",
    "vocabulary = set(words)\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Get most and least frequent words\n",
    "most_frequent = word_counts.most_common(10)\n",
    "least_frequent = word_counts.most_common()[:-11:-1]  # Last 10 in reverse order\n",
    "\n",
    "# Report statistics\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"\\nTotal words in corpus: {len(words)}\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"10 Most Frequent Words:\")\n",
    "print(f\"{'='*50}\")\n",
    "for word, count in most_frequent:\n",
    "    print(f\"{word:20s} : {count:6d}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"10 Least Frequent Words:\")\n",
    "print(f\"{'='*50}\")\n",
    "for word, count in least_frequent:\n",
    "    print(f\"{word:20s} : {count:6d}\")\n",
    "\n",
    "# Save vocabulary and word counts for later use\n",
    "vocab_data = {\n",
    "    'vocabulary': list(vocabulary),\n",
    "    'word_counts': dict(word_counts),\n",
    "    'vocab_size': vocab_size\n",
    "}\n",
    "\n",
    "with open('vocab_data1.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_data, f)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Vocabulary data saved to 'vocab_data1.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mappings for 113644 words\n"
     ]
    }
   ],
   "source": [
    "# Create word to index and index to word mappings\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(f\"Created mappings for {len(word_to_idx)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 759636 training sequences\n",
      "Sequence shape: torch.Size([759636, 3])\n",
      "Target shape: torch.Size([759636])\n",
      "\n",
      "Training samples: 607708\n",
      "Validation samples: 151928\n"
     ]
    }
   ],
   "source": [
    "# Create training sequences (context window = 3 words to predict next word)\n",
    "def create_sequences(words, window_size=3):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(words) - window_size):\n",
    "        seq = words[i:i+window_size]\n",
    "        target = words[i+window_size]\n",
    "        \n",
    "        # Convert to indices\n",
    "        seq_indices = [word_to_idx[word] for word in seq]\n",
    "        target_idx = word_to_idx[target]\n",
    "        \n",
    "        sequences.append(seq_indices)\n",
    "        targets.append(target_idx)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "window_size = 3\n",
    "X, y = create_sequences(words, window_size)\n",
    "X = torch.tensor(X).to(device)\n",
    "y = torch.tensor(y).to(device)\n",
    "\n",
    "print(f\"Created {len(X)} training sequences\")\n",
    "print(f\"Sequence shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP Text Generator Model with configurable activation function\n",
    "class MLPTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, window_size, \n",
    "                 activation_fn, dropout_prob=0.3):\n",
    "        super(MLPTextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.input_dim = embedding_dim * window_size\n",
    "        \n",
    "        # Map string to actual activation function\n",
    "        self.activation = self._get_activation(activation_fn)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def _get_activation(self, activation_fn):\n",
    "        \"\"\"Return activation function module given string name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'leakyrelu': nn.LeakyReLU(0.01),\n",
    "            'gelu': nn.GELU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'selu': nn.SELU(),\n",
    "            'none': nn.Identity()\n",
    "        }\n",
    "        if activation_fn.lower() not in activations:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_fn}\")\n",
    "        return activations[activation_fn.lower()]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, window_size)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(x)  # (batch_size, window_size, embedding_dim)\n",
    "        \n",
    "        # Flatten embeddings\n",
    "        embedded = embedded.view(batch_size, -1)  # (batch_size, window_size * embedding_dim)\n",
    "        \n",
    "        # MLP forward\n",
    "        h1 = self.dropout1(self.activation(self.fc1(embedded)))\n",
    "        output = self.fc2(h1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 120,221,036\n",
      "Trainable parameters: 120,221,036\n",
      "\n",
      "Model architecture:\n",
      "MLPTextGenerator(\n",
      "  (embedding): Embedding(113644, 32)\n",
      "  (activation): Tanh()\n",
      "  (fc1): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=113644, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model1 = MLPTextGenerator(vocab_size, embedding_dim=32, hidden_dim=1024, window_size=3, activation_fn='tanh')\n",
    "model1 = model1.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model1.parameters())\n",
    "trainable_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\\n{model1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 123,955,948\n",
      "Trainable parameters: 123,955,948\n",
      "\n",
      "Model architecture:\n",
      "MLPTextGenerator(\n",
      "  (embedding): Embedding(113644, 64)\n",
      "  (activation): Tanh()\n",
      "  (fc1): Linear(in_features=192, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=113644, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model2 = MLPTextGenerator(vocab_size, embedding_dim=64, hidden_dim=1024, window_size=3, activation_fn='tanh')\n",
    "model2 = model2.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model2.parameters())\n",
    "trainable_params = sum(p.numel() for p in model2.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\\n{model2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124,087,020\n",
      "Trainable parameters: 124,087,020\n",
      "\n",
      "Model architecture:\n",
      "MLPTextGenerator(\n",
      "  (embedding): Embedding(113644, 64)\n",
      "  (activation): ReLU()\n",
      "  (fc1): Linear(in_features=320, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=113644, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model3 = MLPTextGenerator(vocab_size, embedding_dim=64, hidden_dim=1024, window_size=5, activation_fn='relu')\n",
    "model3 = model3.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model3.parameters())\n",
    "trainable_params = sum(p.numel() for p in model3.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\\n{model3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124,087,020\n",
      "Trainable parameters: 124,087,020\n",
      "\n",
      "Model architecture:\n",
      "MLPTextGenerator(\n",
      "  (embedding): Embedding(113644, 64)\n",
      "  (activation): Tanh()\n",
      "  (fc1): Linear(in_features=320, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=113644, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model4 = MLPTextGenerator(vocab_size, embedding_dim=64, hidden_dim=1024, window_size=5, activation_fn='tanh')\n",
    "model4 = model4.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model4.parameters())\n",
    "trainable_params = sum(p.numel() for p in model4.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\\n{model4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 120,286,572\n",
      "Trainable parameters: 120,286,572\n",
      "\n",
      "Model architecture:\n",
      "MLPTextGenerator(\n",
      "  (embedding): Embedding(113644, 32)\n",
      "  (activation): ReLU()\n",
      "  (fc1): Linear(in_features=160, out_features=1024, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=113644, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model5= MLPTextGenerator(vocab_size, embedding_dim=32, hidden_dim=1024, window_size=5, activation_fn='relu')\n",
    "model5 = model5.to(device)\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model5.parameters())\n",
    "trainable_params = sum(p.numel() for p in model5.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\\n{model5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_epoch(model, X_train, y_train, optimizer, criterion, batch_size=128):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "    \n",
    "    num_batches = len(X_train) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        batch_X = torch.LongTensor(X_train_shuffled[start_idx:end_idx]).to(device)\n",
    "        batch_y = torch.LongTensor(y_train_shuffled[start_idx:end_idx]).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation function\n",
    "def validate(model, X_val, y_val, criterion, batch_size=128):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    num_batches = len(X_val) // batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            batch_X = torch.LongTensor(X_val[start_idx:end_idx]).to(device)\n",
    "            batch_y = torch.LongTensor(y_val[start_idx:end_idx]).to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[model1, model2, model3, model4, model5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n",
      "Training model with window size: 3\n",
      "============================================================\n",
      "Epoch [25/300]\n",
      "  Train Loss: 3.0642 | Train Acc: 39.22%\n",
      "  Val Loss:   7.0935 | Val Acc:   21.74%\n",
      "============================================================\n",
      "Epoch [50/300]\n",
      "  Train Loss: 2.4739 | Train Acc: 46.51%\n",
      "  Val Loss:   7.4066 | Val Acc:   24.48%\n",
      "============================================================\n",
      "Epoch [75/300]\n",
      "  Train Loss: 2.1240 | Train Acc: 51.72%\n",
      "  Val Loss:   7.6499 | Val Acc:   26.11%\n",
      "============================================================\n",
      "Epoch [100/300]\n",
      "  Train Loss: 1.8860 | Train Acc: 55.90%\n",
      "  Val Loss:   7.8765 | Val Acc:   27.10%\n",
      "============================================================\n",
      "Epoch [125/300]\n",
      "  Train Loss: 1.7221 | Train Acc: 59.02%\n",
      "  Val Loss:   8.0780 | Val Acc:   27.66%\n",
      "============================================================\n",
      "Epoch [150/300]\n",
      "  Train Loss: 1.5929 | Train Acc: 61.67%\n",
      "  Val Loss:   8.2562 | Val Acc:   28.09%\n",
      "============================================================\n",
      "Epoch [175/300]\n",
      "  Train Loss: 1.4993 | Train Acc: 63.62%\n",
      "  Val Loss:   8.4058 | Val Acc:   28.31%\n",
      "============================================================\n",
      "Epoch [200/300]\n",
      "  Train Loss: 1.4242 | Train Acc: 65.26%\n",
      "  Val Loss:   8.5469 | Val Acc:   28.52%\n",
      "============================================================\n",
      "Epoch [225/300]\n",
      "  Train Loss: 1.3648 | Train Acc: 66.55%\n",
      "  Val Loss:   8.6568 | Val Acc:   28.73%\n",
      "============================================================\n",
      "Epoch [250/300]\n",
      "  Train Loss: 1.3174 | Train Acc: 67.59%\n",
      "  Val Loss:   8.7515 | Val Acc:   28.86%\n",
      "============================================================\n",
      "Epoch [275/300]\n",
      "  Train Loss: 1.2767 | Train Acc: 68.52%\n",
      "  Val Loss:   8.8331 | Val Acc:   28.91%\n",
      "============================================================\n",
      "Epoch [300/300]\n",
      "  Train Loss: 1.2431 | Train Acc: 69.24%\n",
      "  Val Loss:   8.9058 | Val Acc:   28.98%\n",
      "============================================================\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: inf\n",
      "Starting training...\n",
      "============================================================\n",
      "Training model with window size: 3\n",
      "============================================================\n",
      "Epoch [25/300]\n",
      "  Train Loss: 2.6277 | Train Acc: 44.62%\n",
      "  Val Loss:   6.8064 | Val Acc:   23.93%\n",
      "============================================================\n",
      "Epoch [50/300]\n",
      "  Train Loss: 2.0471 | Train Acc: 52.95%\n",
      "  Val Loss:   7.1957 | Val Acc:   26.39%\n",
      "============================================================\n",
      "Epoch [75/300]\n",
      "  Train Loss: 1.7050 | Train Acc: 59.06%\n",
      "  Val Loss:   7.5130 | Val Acc:   27.67%\n",
      "============================================================\n",
      "Epoch [100/300]\n",
      "  Train Loss: 1.4847 | Train Acc: 63.53%\n",
      "  Val Loss:   7.7883 | Val Acc:   28.41%\n",
      "============================================================\n",
      "Epoch [125/300]\n",
      "  Train Loss: 1.3324 | Train Acc: 66.80%\n",
      "  Val Loss:   8.0220 | Val Acc:   28.78%\n",
      "============================================================\n",
      "Epoch [150/300]\n",
      "  Train Loss: 1.2253 | Train Acc: 69.14%\n",
      "  Val Loss:   8.2127 | Val Acc:   29.01%\n",
      "============================================================\n",
      "Epoch [175/300]\n",
      "  Train Loss: 1.1490 | Train Acc: 70.81%\n",
      "  Val Loss:   8.3764 | Val Acc:   29.18%\n",
      "============================================================\n",
      "Epoch [200/300]\n",
      "  Train Loss: 1.0890 | Train Acc: 72.15%\n",
      "  Val Loss:   8.5056 | Val Acc:   29.28%\n",
      "============================================================\n",
      "Epoch [225/300]\n",
      "  Train Loss: 1.0461 | Train Acc: 73.15%\n",
      "  Val Loss:   8.6069 | Val Acc:   29.40%\n",
      "============================================================\n",
      "Epoch [250/300]\n",
      "  Train Loss: 1.0077 | Train Acc: 73.97%\n",
      "  Val Loss:   8.6908 | Val Acc:   29.40%\n",
      "============================================================\n",
      "Epoch [275/300]\n",
      "  Train Loss: 0.9779 | Train Acc: 74.62%\n",
      "  Val Loss:   8.7567 | Val Acc:   29.46%\n",
      "============================================================\n",
      "Epoch [300/300]\n",
      "  Train Loss: 0.9511 | Train Acc: 75.22%\n",
      "  Val Loss:   8.8109 | Val Acc:   29.54%\n",
      "============================================================\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: inf\n",
      "Starting training...\n",
      "============================================================\n",
      "Training model with window size: 5\n",
      "============================================================\n",
      "Epoch [25/300]\n",
      "  Train Loss: 1.8309 | Train Acc: 57.74%\n",
      "  Val Loss:   7.5965 | Val Acc:   26.78%\n",
      "============================================================\n",
      "Epoch [50/300]\n",
      "  Train Loss: 1.3402 | Train Acc: 66.30%\n",
      "  Val Loss:   8.0974 | Val Acc:   28.49%\n",
      "============================================================\n",
      "Epoch [75/300]\n",
      "  Train Loss: 1.0511 | Train Acc: 72.19%\n",
      "  Val Loss:   8.4600 | Val Acc:   29.44%\n",
      "============================================================\n",
      "Epoch [100/300]\n",
      "  Train Loss: 0.8521 | Train Acc: 76.64%\n",
      "  Val Loss:   8.8146 | Val Acc:   29.89%\n",
      "============================================================\n",
      "Epoch [125/300]\n",
      "  Train Loss: 0.7101 | Train Acc: 80.01%\n",
      "  Val Loss:   9.2133 | Val Acc:   30.15%\n",
      "============================================================\n",
      "Epoch [150/300]\n",
      "  Train Loss: 0.6032 | Train Acc: 82.61%\n",
      "  Val Loss:   9.5982 | Val Acc:   30.36%\n",
      "============================================================\n",
      "Epoch [175/300]\n",
      "  Train Loss: 0.5240 | Train Acc: 84.58%\n",
      "  Val Loss:   9.9791 | Val Acc:   30.50%\n",
      "============================================================\n",
      "Epoch [200/300]\n",
      "  Train Loss: 0.4609 | Train Acc: 86.22%\n",
      "  Val Loss:   10.3667 | Val Acc:   30.48%\n",
      "============================================================\n",
      "Epoch [225/300]\n",
      "  Train Loss: 0.4101 | Train Acc: 87.48%\n",
      "  Val Loss:   10.7260 | Val Acc:   30.49%\n",
      "============================================================\n",
      "Epoch [250/300]\n",
      "  Train Loss: 0.3692 | Train Acc: 88.55%\n",
      "  Val Loss:   11.1082 | Val Acc:   30.52%\n",
      "============================================================\n",
      "Epoch [275/300]\n",
      "  Train Loss: 0.3358 | Train Acc: 89.44%\n",
      "  Val Loss:   11.4663 | Val Acc:   30.54%\n",
      "============================================================\n",
      "Epoch [300/300]\n",
      "  Train Loss: 0.3071 | Train Acc: 90.22%\n",
      "  Val Loss:   11.8332 | Val Acc:   30.59%\n",
      "============================================================\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: inf\n",
      "Starting training...\n",
      "============================================================\n",
      "Training model with window size: 5\n",
      "============================================================\n",
      "Epoch [25/300]\n",
      "  Train Loss: 2.1572 | Train Acc: 53.66%\n",
      "  Val Loss:   6.6930 | Val Acc:   25.41%\n",
      "============================================================\n",
      "Epoch [50/300]\n",
      "  Train Loss: 1.5437 | Train Acc: 63.30%\n",
      "  Val Loss:   7.1066 | Val Acc:   27.66%\n",
      "============================================================\n",
      "Epoch [75/300]\n",
      "  Train Loss: 1.1917 | Train Acc: 69.98%\n",
      "  Val Loss:   7.4695 | Val Acc:   28.63%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "i=1\n",
    "for model in models:\n",
    "\n",
    "    embedding_dim = model.embedding.embedding_dim\n",
    "    window_size_model = model.input_dim // embedding_dim\n",
    "\n",
    "    # Recreate sequences for the current model's window size\n",
    "    X, y = create_sequences(words, window_size_model)\n",
    "    X = torch.tensor(X).to(device)\n",
    "    y = torch.tensor(y).to(device)\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 300\n",
    "    batch_size = 32768\n",
    "\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Training model with window size: {window_size_model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, X_train.cpu().numpy(), y_train.cpu().numpy(), optimizer, criterion, batch_size)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, X_val.cpu().numpy(), y_val.cpu().numpy(), criterion, batch_size)\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), f'mlp_text_model_dataset2var{i}.pth')\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
